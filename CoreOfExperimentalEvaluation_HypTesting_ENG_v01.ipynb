{
  "cells": [
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": " <img style=\"float: center; width: 80%\" src=\"https://raw.githubusercontent.com/andrejkk/TalksImgs/master/FrontSlideUpperBan.png\">\n<img style=\"float: center; width: 30%\" src=\"https://raw.githubusercontent.com/andrejkk/TalksImgs/master/logo_UL-ICT.png\">\n<p style=\"margin-bottom:6em;\"></p>\n\n\n\n<center>\n    <H1> Evaluation of experiments and statistical hypothesis testing </H1>\n   \n\n<br>\n    <H3> Andrej Košir, Lucami, FE </H4>\n    \n    <H4> Contact: prof. dr. Andrej Košir, andrej.kosir@lucami.fe.uni-lj.si, skype=akosir_sid </H4>\n</center>\n\n\n<p style=\"margin-bottom:12em;\"></p>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Selected research work\n\n\n<img style=\"float: center; width: 100%\" src=\"https://raw.githubusercontent.com/andrejkk/TalksImgs/master/SelectedResearchWork.png\">\n\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "<p style=\"margin-top:3cm;\"></p>\n<center><H1>\nWhere the need for statistical hypothesis testing comes from?\n</H1></center>\n\n<br><br><br>\n<center><H1>\n    From <br><br> \n    randomness in test data <br> <br>\n    and <br><br>\n    proper evaluation of Experimental designs.\n</H1></center>\n\n<p style=\"margin-bottom:4cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 1 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# The structure of the talk\n\n1. Basics of statistical hypothesis testing\n2. Power analysis (sample size, achieved power)\n3. Selection of tests: \n   - on distributions \n   - on assumption verification\n   - test selection\n4. Alternatives to hypothesis testing?\n5. Take away notes\n   \n\n<p style=\"margin-bottom:1cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 2 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# What one needs to know to perform the test\n\n\n1. Design the experiment\n2. Select the most powerfull test (if not yet decided)\n3. Determine the required sample / data size\n4. Verify test assumptions and, if necessary, go back to 2.\n5. Determine p value\n\n<br>\n<center>\n<H1> What are major issues in testing procedure? </H1>\n</center>\n\n\n<p style=\"margin-bottom:1cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 3 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Statistical hypothese testing\n\n#### Problem: decide if the effect is a coincidence or not\n\n- When we need it: when there is a source of randomness in data: noised measurments, human behaviour  \n\n\n\n#### Solution: statistical significance testing\n\n- null hypotheses $H_0$: no effect\n- p-value\n$$ p = Pr[\\mbox{experimental data is this or more deviated from $H_0$}\\Big|_{H_0}] $$\n- risk level $\\alpha$: what you are prepared to believe? \n- conclussion: $p$ < $\\alpha \\Rightarrow H_0 \\;\\mbox{rejected}$. The **effect is significant**.\n\n\n- statistical test is a package of \n    - null hypotheses $H_0$\n    - formula to compute p value\n    - assumptions when it works\n    \n    \n- selection of the appropriate test \n    - hierarchy of tests\n    - the most powerful that meets assumptions\n\n\n<p style=\"margin-bottom:1cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 4 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Errors in statistical hypothese testing\n\n### Error in testing\n\nWhile making a conclusion on $H_0$ there are the following four options:\n\n<table style=\"width:25%\">\n  <tr>\n    <th width=\"16\">  </th>\n    <th> $\\hat{H_0}$ </th>\n    <th> $\\neg\\hat{H_0}$ </th>\n  </tr>\n  <tr>\n    <th> $H_0$ </th>\n    <th> OK </th>\n    <th> err. t. I. </th>\n  </tr>\n  <tr>\n    <th> $\\neg H_0$ </th>\n    <th> err. t. II. </th>\n    <th> OK </th>\n  </tr>\n</table>\n\nErrors **can not be avoided**. \n\n\n#### Error type  I. \nProbability is\n$$ P[\\mbox{err. t. I.}] = \\alpha \\ne f(n) , $$\nthat is exactly risk level $\\alpha$. It is **independent of sample size $n$**. \n\nIn practice this means that while repeating the experience $N$ times, approximately $\\alpha \\cdot N$ times the test will reject $H_0$ when it is true. For instance, if $\\alpha=0.05$ and $N=60$ times, we would reject $H_0$ approximately $0.05\\cdot 60 = 3$ times when it is true. \n\n#### Error type II. \nProbaility is \n$$ P[\\mbox{err. t. II.}] = \\beta = f(n) $$\nIt is **dependent of sample size $n$** and $1-\\beta$ is test power. \n\n\n### Bonferroni correction\n\nIf you perfom $N$ tests to make **one single conclusion** on a hypthesis, for risk level $\\alpha$ you should compare p-value not to $\\alpha$, but to (the next is Šidak correction)\n$$ \\frac{\\alpha}{N} \\approx 1-(1-\\alpha)^{1/N}   $$\n\n\n<p style=\"margin-bottom:1cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 5 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Determine required sample / data size - apriori power analysis \n\n### Problem: What is the required sample size $n$ to make a conclusion?\n\n### Solution: apriori power analysis\n\n\n<br><br>\n##### Effect size: a normalised size of departure from $H_0$. \n$\\def\\ovr#1{{\\overline{#1}}}$\n$\\def\\s{{\\sigma}}$\n\nSpecific for each test. For t-test it is \n$$ es = \\frac{\\ovr{x}_2-\\ovr{x}_1}{\\s_p}, \\qquad \\s_p = \\sqrt{(\\s_1^2 + \\s_2^2)/2}. $$\n\n#### The power curve\n\n<img style=\"center; width: 40%\" src=\"https://raw.githubusercontent.com/andrejkk/TalksImgs/master/StatPower_PowerCurve.png\">\n\n\n<p style=\"margin-bottom:1cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 6 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Sensitivity of the test: posteriori power analysis\n\n### Problem: How to determine the obtained power\n\n### Solution: posteriory power analyis\n\n\n#### What is sensitivity: the ability to detect small effects. \n\n\n<br>\n#### Effect size $es$:\n\nEffect size $es$ is estimated from **this study data.**\n\nUse **tools** like GPower or even **tables**.\n\n**p-value** is **not a measure of effect size!** Low p-value does not mean high effect size.  \n\n\n<br>\n#### Power form effect size\n\n<img style=\"center; width: 40%\" src=\"https://raw.githubusercontent.com/andrejkk/TalksImgs/master/PowerVsEffectSize.png\">\n\n\n\n<p style=\"margin-bottom:1cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 7 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# A gap between significant and useful\n\n<img style=\"right; width: 60%\" src=\"https://raw.githubusercontent.com/andrejkk/TalksImgs/master/StatPower_EffectSize.png\">\n\n\n<p style=\"margin-bottom:1cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 8 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# On significance  test assumption verification\n\n\n#### Why: formulas for p value returns wrong result otherwise. \n\nThe derivation of formulas relied on it. \n\n\n#### Assumption types:\nBeside checking variable type, we have\n1. Minimal values: like at leasst 5 in each cell of $\\chi^2$ test. Trivial to check.\n2. Deviation from distributions - mostly from Normal one.\n3. Homogenity of variance etc.\n\n\n#### Do assumptions fails \"yes / no\" or continiously \"more or less\"? \n\nContiniously. But statistical tests decides them in a \"yes/no\" manner. \n \n\n\n<p style=\"margin-bottom:1cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 9 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# On probability distributions - where probability distributions come from (1)?\n\n### Distsributions are **determined by the process of data generation!**\n\nDenote\n$$ \\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i $$\n\n\n\n<br><br>\n### Different conditions in summing up a lot of contributions (limit theorems)\n\n\n##### Average of independent normal variables with known mean $\\mu$ and known standard deviation $\\sigma$: Normal\n\n$$ X_i\\sim N(a,\\sigma), \\qquad\\Rightarrow\\qquad \\bar{X}_n \\sim N(\\mu, \\sigma/n), $$\n\n\n<br>\n##### Average of independent normal variables with known mean $\\mu$ and unknown standard deviation $\\sigma$: Student t\n$$ X_i\\sim N(\\mu,\\sigma), \\qquad\\Rightarrow\\qquad \\frac {{\\bar{X}_n}-\\mu }{S/{\\sqrt {n}}} \\sim t(n-1), $$\nwhere $S^2 = \\frac{1}{n-1}\\sum_i (x_i-\\overline{x})^2$. It is true for low sample sizes, with large sample sizes it goes to Normal.\n\n<br>\n##### Constant expected value of Binomials: Poisson\nIf $np$ is constant (they are not independent), we **do not get normal distribution** - a model of ordinary traffic. \n\n<br>\n##### ...\n\n\n\n\n\n<p style=\"margin-bottom:1cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 9 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# On probability distributions - where probability distributions come from (2)?\n\nDenote\n$$ \\bar{X}_n = \\frac{1}{n} \\sum_{i=1}^n X_i $$\n\n\n\n### What the analysis needs\n\n\n##### Sum of squared standardized normal distributed: $\\chi$ squared\n$$ X_i\\sim N(0,1), \\qquad\\Rightarrow\\qquad X_1^2 + \\cdots + X_d^2 \\sim \\chi^2(d) $$\n\n\n##### Quotient of $\\chi^2$: F-distribution \n$$ X_1 \\sim \\chi^2(d_1),  X_2 \\sim \\chi^2(d_2) \\qquad\\Rightarrow\\qquad \\quad \\frac{X_1/d_1}{X_2/d_2} \\sim F(d_1, d_2). $$\n\n<p style=\"margin-bottom:1cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 10 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Structure of Normal distribution assumption - what can go wrong (1)\n\n### Variable type\n\nNormal distribution makes sense on interval and proporional variables, that is continious variables.\n\n##### How severe: severe\n\n\n### Is unimodal?\n\n<img style=\"right; width: 50%\" src=\"https://raw.githubusercontent.com/andrejkk/TalksImgs/master/Distributions_MultiModal.png\">\n\n##### How severe: very severe\n\n### How to check it?\n\nPlot data. \n\n<p style=\"margin-bottom:0.5cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 11 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Structure of Normal distribution assumption - what can go wrong (2)\n\n\n\n### Is symmetric?\n\n<img style=\"right; width: 50%\" src=\"https://raw.githubusercontent.com/andrejkk/TalksImgs/master/Distributions_NonSymetric.png\">\n\n##### How severe: severe\n\n\n### How to check it?\n\nTest skewness. \n\n\n<p style=\"margin-bottom:1cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 12 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Structure of Normal distribution assumption - what can go wrong (3)\n\n\n\n### Has exponential tails? \n\n\n<img style=\"right; width: 50%\" src=\"https://raw.githubusercontent.com/andrejkk/TalksImgs/master/Distributions_Tails.png\">\n\n##### How severe: not problematic\n\n\n\n### How to check it?\n\n1. Plot quantiles\n2. Rojo procedure\n\n\n<p style=\"margin-bottom:1cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 13 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Distributon of variables and distribution of data (1)\n\n\n\n##### Must not confuse\nProbability distribution of \n- random variable $X$ and\n- data generated from $X$\n**may not be the same.**\n\n\n\n##### Some facts:\n- random variable distribution matters for statistical tests\n- only data distribution is verifiable\n\n\n##### A key difference: \n- if data is sampled without quantisations/encoding or restrictions, those two are the same\n- if you have random variable X distributed normally, but you encode it by 0 and 1, it is not the same. Therefore, $0$ and $1$ from Bernoully distribution is different than $0$ and $1$ from encoded normal distribution\n\n\n<p style=\"margin-bottom:1cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 14 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Distributon of variables and distribution of data (2)\n\n$\\def\\s{{\\sigma}}$\n$\\def\\ovr#1{{\\overline{#1}}}$\n\n\n### A case of t-test\n- we need to make sure the RANDOM VARIABLE is normal. \n- if we have OK sampling, we use Lilliefors test or similar **on the data sample**\n- if sampling is not OK and we have, for instance $0$ and $1$, it is a new story requiring specific reasoning;\n- **not all agree:** Some say that ordinal is OK. \n\n\n\n\n### t-test assumptions\n- measurement applied to the data collected follows a continuous or ordinal scale\n- randomly selected (no strategy, no encoding!)\n- data is normaly distributed: (unskewed & exponential tails)\n- sample large enough: 12, 15, ... more is better\n- homogenous variance: equal variance for low and high values...\n- --> ordinal do good\n\n\n\n<p style=\"margin-bottom:1cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 15 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Selection of statistical test\n\n\nTwo competitive features\n- assumptions are met\n- most powerfull test\n\n\n<img style=\"right; width: 60%\" src=\"https://raw.githubusercontent.com/andrejkk/TalksImgs/master/HierarchyOfStatTests.png\">\n\n<br>\n<p style=\"margin-bottom:1cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 17 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Is there an alternative to Hypothese testing? \n\n#### Yes, there is \n\n<center><H1>\nBayesian inference\n</H1></center>\n\n<br>\nGet rid of risk of errors in conclusion? No.\n\nGet rid of risk level determination problem? No.  Instead of $p$ < $\\alpha$ we have _“probability to be best” goes above a threshold or the expected loss is below a threshold_. \n\nWhy not much in use? Matehmatically more challenging! You need to know about \"Posterior, Bayes Theorem, Beta and Gamma distributions, Monte Carlo Integration, Highest Posterior Density Region (HPDR)\"\n\n\n#### Why bother with it then?\n\nIt gives **additional insight into the problem**?\n\nDoable for non-mathematician data scientist? **YES**\n\nYou should considered it!\n\n\n\n\n\n\n\n\n<p style=\"margin-bottom:1cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 18 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Notes and recommendations\n\n\n\n### The problem of small values in proportion test - at the edges of distributions\n\nThe estimation of p-value might be wrong.\n\n\n### The problem of large sample sizes\n\nEverything is significant. \n\n\n### Learn Bayesian inference\n\nIt gives further insights.\n\n<p style=\"margin-bottom:1cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 19 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# \n\n\n<p style=\"margin-bottom:1cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 2 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# \n\n\n<p style=\"margin-bottom:1cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 2 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# \n\n\n<p style=\"margin-bottom:1cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 2 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Apendix\n\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "## Finding the size of the sample for hypotheses: apriori power analysis\n\n- types of erros and their probability\n\n\n- effect size - specific for each testing\n    - for instance comparing means: \n    $$ \\frac{\\overline{x_1} - \\overline{x_2}}{\\sigma_{x_1, x_2}} $$\n\n\n- Cohen:\n    - small: $es < 0.2$\n    - medium: $0.2 \\leq es < 0.7$\n    - large: $0.7 \\leq es$\n    \n    \n- From test type and effect size you can get lower limit of your sample size $n_0$ \n    - Software GPower\n    \n    \n\n\n<p style=\"margin-bottom:1cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 2 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "## On types of experimental designs\n\n- Major design is covered by **ANOVA**\n    - What is ANOVA: a scheme of input data, to the F-test (Figure)\n    - Versions: MANOVA, ANCOVA\n    - Names: \n        - one factor: one-way ANOVA\n        - two factors two-way ANOBA\n        - more than two factors: ---> factorial design\n\n\n\n- Interesting design is a **Latin Square**\n    - latin square covers two factors \n    - $n\\times n$ Latin square (figure)\n    - design guide: \n        - what are **two most relevant** factors?\n        - how to group their levels into $n$ values?\n\n\n\n\n<p style=\"margin-bottom:1cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 2 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "## Experimental design\n\n- What is experimental design\n    - nonexperimental designs\n    - experimental design\n    \n  \n- A scheme of experimental design (figure) \n\n\n\n\n- Elements of experimental design\n    - outcome = dependent variable: how we measure sucess\n    - factor(s) = independent variable(s): what we control\n    - nuisance facotr(s) = independent variable(s): what we want to eliminate\n    - possibly additional pure noise\n\n\n\n#### Problem: is the effect size large enough to detect it\n- What is effect size\n\n\n#### Is the progres a coincidence or real?\n- Solution: hypothese testing [Appendix if you chose to have it]\n\n\n\n\n<p style=\"margin-bottom:1cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 2 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "## Finding the size of the sample for parameter estimation: confidece intervals\n\n- the problem of parameter estimation\n\n\n- confidence interval\n$$ CI = [\\hat{\\overline{x}} - 1.96 \\frac{\\hat{\\sigma}}{\\sqrt{n}}, \n\\hat{\\overline{x}} + 1.96 \\frac{\\hat{\\sigma}}{\\sqrt{n}}] $$\n\n\n- determination of sample size\n\n\n\n<p style=\"margin-bottom:1cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 2 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Performing the test: Available softwares\n\n\n#### Typical using software\n\n- noone needs to know equations to calculate p value\n- selection and verification of test assumptions are critical\n\n\n\n\n#### Available softrwares\n1. IBM SPSS\n2. Python libraries\n3. R libraries\n4. LISREL\n5. Matlab, ...\n\n\n<p style=\"margin-bottom:1cm;\"></p>\n<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\"> 2 </div>\n<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\">"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.15",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 2,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}